{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56407e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4846, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'C:\\Users\\LENOVO\\Desktop\\Python\\dataset_for_deepLearning\\sentiment_analysis_for_financial_news\\all-data.xlsx', engine='openpyxl')\n",
    "df.dropna(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "512db0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4145dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiket=df.iloc[:,0:1].values\n",
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "etiket[:,0]=le.fit_transform(df.iloc[:,0])\n",
    "ohe=preprocessing.OneHotEncoder()\n",
    "etiket=ohe.fit_transform(etiket).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb96274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X=df.iloc[:,1:2].values\n",
    "print(type(X))\n",
    "print(type(etiket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c953c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, etiket, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf8404a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3876, 1)\n",
      "(3876, 3)\n",
      "(970, 1)\n",
      "(970, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a36d7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for x in range(0,X_train.shape[0]):\n",
    "    texts.append(X_train[x,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19f201d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9001 unique tokens.\n",
      "Shape of data tensor: (3876, 100)\n",
      "Shape of label tensor: (3876, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "maxlen=100\n",
    "training_samples=3000\n",
    "validation_samples=876\n",
    "max_words=10000\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "word_index=tokenizer.word_index\n",
    "print(\"Found %s unique tokens.\"%len(word_index))\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "labels=np.asarray(y_train)\n",
    "print(\"Shape of data tensor:\",data.shape)\n",
    "print(\"Shape of label tensor:\",labels.shape)\n",
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "labels=labels[indices]\n",
    "x_train=data[:training_samples]\n",
    "y_train=labels[:training_samples]\n",
    "x_val=data[training_samples:training_samples+validation_samples]\n",
    "y_val=labels[training_samples:training_samples+validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebce8842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_dir=\"C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/Imdb/glove.6B\"\n",
    "embeddings_index={}\n",
    "f=open(os.path.join(glove_dir,\"glove.6B.200d.txt\"),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype=\"float32\")\n",
    "    embeddings_index[word]=coefs\n",
    "f.close()\n",
    "print(\"found %s word vectors\"%len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "468ac004",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=200\n",
    "embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<max_words:\n",
    "        embedding_vector=embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a441703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 64)           44736     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               49536     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,094,659\n",
      "Trainable params: 2,094,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, SimpleRNN, LSTM, Bidirectional, GRU\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Bidirectional(GRU(32,dropout=0.5,recurrent_dropout=0.5,return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(64,activation='relu',dropout=0.5,recurrent_dropout=0.5)))\n",
    "model.add(Dense(3,activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e96731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f66d8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "model_path=\"C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.{epoch:02d}-{val_acc:.2f}.h5\"\n",
    "mc =ModelCheckpoint(filepath=model_path, monitor='val_acc',verbose=1, mode=\"max\", save_best_only=True)\n",
    "callback_list=[mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02ba4599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 876 samples\n",
      "Epoch 1/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.9255 - acc: 0.5968Epoch 00000: val_acc improved from -inf to 0.58333, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.00-0.58.h5\n",
      "3000/3000 [==============================] - 39s - loss: 0.9249 - acc: 0.5970 - val_loss: 0.8879 - val_acc: 0.5833\n",
      "Epoch 2/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.8739 - acc: 0.6085Epoch 00001: val_acc improved from 0.58333 to 0.63584, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.01-0.64.h5\n",
      "3000/3000 [==============================] - 28s - loss: 0.8746 - acc: 0.6077 - val_loss: 0.8659 - val_acc: 0.6358\n",
      "Epoch 3/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.8624 - acc: 0.6176Epoch 00002: val_acc did not improve\n",
      "3000/3000 [==============================] - 27s - loss: 0.8611 - acc: 0.6177 - val_loss: 0.8354 - val_acc: 0.6279\n",
      "Epoch 4/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.8359 - acc: 0.6347Epoch 00003: val_acc improved from 0.63584 to 0.64954, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.03-0.65.h5\n",
      "3000/3000 [==============================] - 27s - loss: 0.8360 - acc: 0.6347 - val_loss: 0.7978 - val_acc: 0.6495\n",
      "Epoch 5/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.8104 - acc: 0.6421Epoch 00004: val_acc did not improve\n",
      "3000/3000 [==============================] - 30s - loss: 0.8088 - acc: 0.6430 - val_loss: 0.7788 - val_acc: 0.6427\n",
      "Epoch 6/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7969 - acc: 0.6542Epoch 00005: val_acc did not improve\n",
      "3000/3000 [==============================] - 30s - loss: 0.7965 - acc: 0.6540 - val_loss: 0.7590 - val_acc: 0.6450\n",
      "Epoch 7/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7790 - acc: 0.6603Epoch 00006: val_acc did not improve\n",
      "3000/3000 [==============================] - 25s - loss: 0.7780 - acc: 0.6613 - val_loss: 0.7771 - val_acc: 0.6495\n",
      "Epoch 8/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7689 - acc: 0.6562Epoch 00007: val_acc improved from 0.64954 to 0.67237, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.07-0.67.h5\n",
      "3000/3000 [==============================] - 23s - loss: 0.7687 - acc: 0.6560 - val_loss: 0.7334 - val_acc: 0.6724\n",
      "Epoch 9/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7587 - acc: 0.6630Epoch 00008: val_acc did not improve\n",
      "3000/3000 [==============================] - 28s - loss: 0.7593 - acc: 0.6623 - val_loss: 0.7660 - val_acc: 0.6381\n",
      "Epoch 10/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7397 - acc: 0.6613Epoch 00009: val_acc improved from 0.67237 to 0.67580, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.09-0.68.h5\n",
      "3000/3000 [==============================] - 32s - loss: 0.7389 - acc: 0.6620 - val_loss: 0.7073 - val_acc: 0.6758\n",
      "Epoch 11/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.6657Epoch 00010: val_acc improved from 0.67580 to 0.67694, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.10-0.68.h5\n",
      "3000/3000 [==============================] - 26s - loss: 0.7433 - acc: 0.6653 - val_loss: 0.6994 - val_acc: 0.6769\n",
      "Epoch 12/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.6808Epoch 00011: val_acc did not improve\n",
      "3000/3000 [==============================] - 25s - loss: 0.7098 - acc: 0.6807 - val_loss: 0.7030 - val_acc: 0.6689\n",
      "Epoch 13/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.6771Epoch 00012: val_acc improved from 0.67694 to 0.70434, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.12-0.70.h5\n",
      "3000/3000 [==============================] - 24s - loss: 0.7141 - acc: 0.6773 - val_loss: 0.6729 - val_acc: 0.7043\n",
      "Epoch 14/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.7102 - acc: 0.7013Epoch 00013: val_acc improved from 0.70434 to 0.71119, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.13-0.71.h5\n",
      "3000/3000 [==============================] - 24s - loss: 0.7103 - acc: 0.7010 - val_loss: 0.6583 - val_acc: 0.7112\n",
      "Epoch 15/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6832 - acc: 0.6909Epoch 00014: val_acc did not improve\n",
      "3000/3000 [==============================] - 28s - loss: 0.6828 - acc: 0.6913 - val_loss: 0.6549 - val_acc: 0.7055\n",
      "Epoch 16/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6704 - acc: 0.6999Epoch 00015: val_acc improved from 0.71119 to 0.71918, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.15-0.72.h5\n",
      "3000/3000 [==============================] - 32s - loss: 0.6701 - acc: 0.7000 - val_loss: 0.6368 - val_acc: 0.7192\n",
      "Epoch 17/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6570 - acc: 0.7093Epoch 00016: val_acc improved from 0.71918 to 0.72374, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.16-0.72.h5\n",
      "3000/3000 [==============================] - 30s - loss: 0.6585 - acc: 0.7087 - val_loss: 0.6300 - val_acc: 0.7237\n",
      "Epoch 18/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6508 - acc: 0.7154Epoch 00017: val_acc improved from 0.72374 to 0.73630, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.17-0.74.h5\n",
      "3000/3000 [==============================] - 29s - loss: 0.6487 - acc: 0.7167 - val_loss: 0.6115 - val_acc: 0.7363\n",
      "Epoch 19/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.7238Epoch 00018: val_acc improved from 0.73630 to 0.75457, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.18-0.75.h5\n",
      "3000/3000 [==============================] - 26s - loss: 0.6444 - acc: 0.7237 - val_loss: 0.5947 - val_acc: 0.7546\n",
      "Epoch 20/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6110 - acc: 0.7312Epoch 00019: val_acc did not improve\n",
      "3000/3000 [==============================] - 23s - loss: 0.6120 - acc: 0.7307 - val_loss: 0.6025 - val_acc: 0.7295\n",
      "Epoch 21/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6129 - acc: 0.7419Epoch 00020: val_acc did not improve\n",
      "3000/3000 [==============================] - 25s - loss: 0.6127 - acc: 0.7423 - val_loss: 0.6147 - val_acc: 0.7386\n",
      "Epoch 22/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.6116 - acc: 0.7386Epoch 00021: val_acc improved from 0.75457 to 0.75799, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.21-0.76.h5\n",
      "3000/3000 [==============================] - 32s - loss: 0.6123 - acc: 0.7387 - val_loss: 0.5756 - val_acc: 0.7580\n",
      "Epoch 23/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5872 - acc: 0.7513Epoch 00022: val_acc improved from 0.75799 to 0.77283, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.22-0.77.h5\n",
      "3000/3000 [==============================] - 33s - loss: 0.5866 - acc: 0.7517 - val_loss: 0.5605 - val_acc: 0.7728\n",
      "Epoch 24/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.7544Epoch 00023: val_acc did not improve\n",
      "3000/3000 [==============================] - 31s - loss: 0.5950 - acc: 0.7540 - val_loss: 0.5619 - val_acc: 0.7683\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.7554Epoch 00024: val_acc did not improve\n",
      "3000/3000 [==============================] - 30s - loss: 0.5837 - acc: 0.7560 - val_loss: 0.5483 - val_acc: 0.7683\n",
      "Epoch 26/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5643 - acc: 0.7692Epoch 00025: val_acc did not improve\n",
      "3000/3000 [==============================] - 26s - loss: 0.5633 - acc: 0.7690 - val_loss: 0.5439 - val_acc: 0.7717\n",
      "Epoch 27/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5432 - acc: 0.7759- ETA: 4s - loss: 0.5Epoch 00026: val_acc did not improve\n",
      "3000/3000 [==============================] - 25s - loss: 0.5454 - acc: 0.7760 - val_loss: 0.5466 - val_acc: 0.7705\n",
      "Epoch 28/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5376 - acc: 0.7755Epoch 00027: val_acc improved from 0.77283 to 0.79338, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.27-0.79.h5\n",
      "3000/3000 [==============================] - 29s - loss: 0.5392 - acc: 0.7753 - val_loss: 0.5186 - val_acc: 0.7934\n",
      "Epoch 29/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5354 - acc: 0.7725Epoch 00028: val_acc did not improve\n",
      "3000/3000 [==============================] - 27s - loss: 0.5343 - acc: 0.7727 - val_loss: 0.5358 - val_acc: 0.7740\n",
      "Epoch 30/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5343 - acc: 0.7836Epoch 00029: val_acc improved from 0.79338 to 0.79452, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.29-0.79.h5\n",
      "3000/3000 [==============================] - 28s - loss: 0.5341 - acc: 0.7837 - val_loss: 0.5159 - val_acc: 0.7945\n",
      "Epoch 31/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8004Epoch 00030: val_acc did not improve\n",
      "3000/3000 [==============================] - 26s - loss: 0.5101 - acc: 0.7993 - val_loss: 0.5133 - val_acc: 0.7945\n",
      "Epoch 32/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.7960Epoch 00031: val_acc improved from 0.79452 to 0.79795, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.31-0.80.h5\n",
      "3000/3000 [==============================] - 32s - loss: 0.5055 - acc: 0.7950 - val_loss: 0.5064 - val_acc: 0.7979\n",
      "Epoch 33/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.5078 - acc: 0.7977Epoch 00032: val_acc did not improve\n",
      "3000/3000 [==============================] - 24s - loss: 0.5077 - acc: 0.7977 - val_loss: 0.5104 - val_acc: 0.7922\n",
      "Epoch 34/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4887 - acc: 0.8044Epoch 00033: val_acc did not improve\n",
      "3000/3000 [==============================] - 31s - loss: 0.4887 - acc: 0.8043 - val_loss: 0.5212 - val_acc: 0.7865\n",
      "Epoch 35/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8001Epoch 00034: val_acc improved from 0.79795 to 0.80479, saving model to C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.34-0.80.h5\n",
      "3000/3000 [==============================] - 30s - loss: 0.4885 - acc: 0.8003 - val_loss: 0.5064 - val_acc: 0.8048\n",
      "Epoch 36/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8024Epoch 00035: val_acc did not improve\n",
      "3000/3000 [==============================] - 28s - loss: 0.4745 - acc: 0.8027 - val_loss: 0.5160 - val_acc: 0.7979\n",
      "Epoch 37/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4883 - acc: 0.8071Epoch 00036: val_acc did not improve\n",
      "3000/3000 [==============================] - 30s - loss: 0.4858 - acc: 0.8083 - val_loss: 0.5183 - val_acc: 0.7979\n",
      "Epoch 38/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4906 - acc: 0.8048Epoch 00037: val_acc did not improve\n",
      "3000/3000 [==============================] - 28s - loss: 0.4897 - acc: 0.8050 - val_loss: 0.5122 - val_acc: 0.7968\n",
      "Epoch 39/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4851 - acc: 0.8061Epoch 00038: val_acc did not improve\n",
      "3000/3000 [==============================] - 25s - loss: 0.4840 - acc: 0.8063 - val_loss: 0.5111 - val_acc: 0.7968\n",
      "Epoch 40/40\n",
      "2976/3000 [============================>.] - ETA: 0s - loss: 0.4553 - acc: 0.8199Epoch 00039: val_acc did not improve\n",
      "3000/3000 [==============================] - 26s - loss: 0.4541 - acc: 0.8207 - val_loss: 0.5064 - val_acc: 0.8025\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "history=model.fit(x_train,y_train,\n",
    "                 epochs=40,\n",
    "                 batch_size=32,\n",
    "                 callbacks=callback_list,\n",
    "                 validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27aab26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts=[]\n",
    "for x in range(0,X_test.shape[0]):\n",
    "    test_texts.append(X_test[x,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73938152",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=tokenizer.texts_to_sequences(test_texts)\n",
    "x_test=pad_sequences(sequences,maxlen=maxlen)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "446d45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model=load_model(\"C:/Users/LENOVO/Desktop/Python/dataset_for_deepLearning/sentiment_analysis_for_financial_news/saved_models/weights.34-0.80.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c45e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/970 [============================>.] - ETA: 0stest acc: 0.8030927835051547\n",
      "test loss: 0.5087852184305486\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc=new_model.evaluate(x_test,y_test)\n",
    "print('test acc:', test_acc)\n",
    "print('test loss:',test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88679cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
